---
title: "Remote Network Calibration"
author: "Audrey Smith"
date: "6/9/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(httr)
library(jsonlite)
library(dplyr)
library(lubridate)
library(ggplot2)
```

* This script reads in un-calibrated network data that has been pulled from the Aeroqual API via Python script. 
* This script requires a proxy to calibrate our ozone data. San Pablo Station regulatory data is pulled for these purposes and used as a proxy for all stations, given the regional nature of ozone pollution. Proximity is more important than land use here, justifying the choice of one location for proxy data.
  -Later, will test results of using just San Pablo station versus pulling air monitoring data from nearby monitors, including Aquatic Park, West Oakland, San Rafael, Vallejo, etc.
    -https://www.baaqmd.gov/about-air-quality/current-air-quality/air-monitoring-data/#/airp?id=59&style=chart&zone=-1&date=2020-06-09&view=hourly
    -Feasible some stations may be equidistant between San Pablo and Berkeley Aquatic Park - calculate distance between each monitor and these stations to verify SP closest for all
       -If not, consider integrating Berkeley data for south Richmond stations

1. Read in data pulled daily by Blue's script

2. Call regulatory API and wrangle their data into same format as ours
   *** AirNow API provides unverified regulatory data in real time: https://docs.airnowapi.org/Data/query ***
   * EPA Air Quality System API provides authoritative regulatory data (but up to 6 month lag): https://aqs.epa.gov/aqsweb/documents/data_api.html
   * World AQI collaboration between global EPAs provides regulatory data by station in real time: https://aqicn.org/api/
   ?? Do we know of somewhere to get validated data with less lag? Will have to use AirNow data for now but should have a retroactive check in place maybe
   2.a. Each day, pull in just the previous day and append to a running log of proxy data that is written as output every day

3.a. Run Kolmogorov Smirnov test to compare three-day rolling average of each monitor to that of the BAAQMD data
    3.a.i. A flag should be created for each time KS statistic goes below 0.05 (X = 1)
        3.a.ii. Create separate column for running, five-day count of flag (Y). If X_yesterday == 1 & X_today == 1, Y = sum(Y_yesterday+Y_today). Else, Y_today = 0.

3.b. Run deviance test to see if mean and variance of our data differ substantially with those of the proxy data
    3.b.i. A separate flag should be created for each of these, follow same scheme for triggering re-calibration as detailed under 3.a.
    
3.c. Each day, write a running .csv with flag results
    3.c.i. Next day, read in just one day of new data and append to the previous day's running file

4. If Y < 5, pass. If Y == 5, re-calibrate.
    4.a. Calibration uses mean and variance of our data, proxy data to create new slope and offset
        * I understand the calibration to be applied to day 6 onward, need to verify that it doesn't begin to apply to day 5 or apply retroactively at day 1
        ** Calibration uses mean and variance for 30-day period, beginning on day 5 and going backwards
        
5. Need to create log of what day calibration parameter changed, how many parameters were created for any given monitor, and what they were
    5.i. Create separate CSV logging these values for each monitor

_Assumption Checking_ (Miskell et al. 2018)
1. Sensor data are a linear transformation of true data
2. Frequency distribution of site and proxy data have similar functional form over suitable time period
3. 

_Drift Detection_
```{r}
### DOWNLOAD PROXY DATA
# Get last three days of regulatory data  near Richmond
end_date <- Sys.Date()-2 #Yesterday
start_date <- Sys.Date()-4 #Three days ago

request_url = paste0('http://www.airnowapi.org/aq/data/?startDate=', start_date, 'T00&endDate=', end_date, 'T23&parameters=OZONE,PM25,NO2&BBOX=-122.720038,37.701918,-121.965229,38.126622&dataType=C&format=application/json&verbose=0&nowcastonly=0&includerawconcentrations=1&API_KEY=C05358E3-5508-4216-A03E-E229E0368B7E') # Set URL to request data from last three days from AirNow API ## DATES NEEDED AS FUNCTION INPUT HERE
request_call <- GET(url = request_url) #Request data from API
stop_for_status(request_call) # Converts API error to R error - fails if invalid query or data unavailable
request_json <- content(request_call, as = 'text', type = NULL, encoding = 'UTF-8') # Get JSON content from API request as text
request_df <- fromJSON(request_json, simplifyDataFrame = TRUE) # Parse JSON to dataframe

# Filter to O3 and join with station names, filter to San Pablo
reg_names <- read.csv('shapefiles/monitor_shp_named.csv') %>% select(-X) %>% rename('station'='monitor_names') # Read in names of each regulatory station near Richmond
reg_data <- inner_join(request_df, reg_names, by = c('Latitude'='monitor_lat', 'Longitude'='monitor_long')) # Join station names with data by lat/long

reg_data$RawConcentration <- ifelse(reg_data$RawConcentration == -999, reg_data$Value, reg_data$RawConcentration) # If raw concentration unavailable, use rounded value from "value" field
reg_data$time_stamp <- as_datetime(paste(substr(reg_data$UTC, 1, 10), substr(reg_data$UTC, 12, 16), ':00'))

sp_O3 <- filter(reg_data, Parameter == 'OZONE' & station == 'San Pablo') # Filter to pollutant and station of interest 
bk_O3 <- filter(reg_data, Parameter == 'OZONE' & station == 'Berkeley')
```

For now we will average the minute data into sixty minute increments for more comparable sample size to proxy data - do we have access to better temporal granularity for proxy data?
```{r}
### READ IN PSE DATA FROM DAY PRIOR
aqy_data_full <- read.csv(paste0('test_data/AQY_June', '.CSV'), stringsAsFactors = F) # Read in our air data ## DATES AS INPUTS
aqy_cal_params <- read.csv('calParams_sac.csv') # Read in present calibration parameters

### DELETE THIS LINE LATER - testing with two closest to SP Station (first two) and two that are closer to Berkeley than SP
aqy_data_full <- aqy_data_full %>% filter(ID %in% c('AQY BB-635', 'AQY BB-640', 'AQY BB-793', 'AQY BB-648'))

#Filtering to last three days
aqy_data_full$time_stamp <- as_datetime(aqy_data_full$Time) # Get timestamp into standard format
aqy_data <- filter(aqy_data_full, time_stamp >= start_date & time_stamp <= end_date) # Filter to timestamps within last 72 hours

#Calibrating using current parameters
aqy_data_cal <- inner_join(aqy_cal_params, aqy_data, by = c('Monitor.ID'='ID')) # Join uncalibrated data with present calibration parameters
aqy_data_cal$O3_cal <- (aqy_data_cal$O3-aqy_data_cal$O3.offset)*aqy_data_cal$O3.gain # Calibrate O3 data

# Generating list of AQYs
aqy_list <- as.data.frame(unique(aqy_data_cal$Monitor.ID)) # Generate list of AQY instruments with measurements in time frame
colnames(aqy_list) <- 'Monitor.ID'
```

Miskell et al. 2016 indicates:
slope = sqrt(variance_proxy/variance_aqy)
offset = mean_aqy-mean_proxy*sqrt(variance_proxy/variance_aqy)

Says drift has occurred if 0.7 < slope < 1.3 or -5 < offset < 5 ??Are the greater than/less than signs flipped??

```{r}
### GENERATE FLAGS
# Initialize dataframe of today's flags
todays_flags <- mutate(aqy_list, flag_KS_now = 0, flag_slope_now = 0, flag_offset_now = 0)

# Initialize dataframe of today's test results
todays_results <- mutate(aqy_list, start_72hr = start_date, end_72hr = end_date, ks_pval = NA, offset = NA, slope = NA)

i <- 1

while(i <= nrow(aqy_list)){
  aqy_O3 <- filter(aqy_data_cal, Monitor.ID == aqy_list[i,1]) # Select AQY of interest
  aqy_ID <- unique(aqy_O3$Monitor.ID) # Get monitor ID we're working with
  
  #Averaging by hour
  aqy_O3$day_hour <- paste(day(aqy_O3$Time), hour(aqy_O3$Time)) # Generate column with unique ID for day/hour
  aqy_O3_60 <- group_by(aqy_O3, Monitor.ID, day_hour) %>% summarize(O3 = mean(O3_cal)) # Group by day/hour and take average of minute measurements
  
  ## Flag via KS test
  ks_results_bk <- ks.test(bk_O3$RawConcentration, aqy_O3_60$O3, alternative = 'two.sided') # Compare distribution to Berkeley Marina
  ks_results_sp <- ks.test(sp_O3$RawConcentration, aqy_O3_60$O3, alternative = 'two.sided') # Compare distribution to San Pablo
  
  ks_p <- ifelse(aqy_ID == 'AQY BB-793' | aqy_ID == 'AQY BB-648', ks_results_bk$p.value, ks_results_sp$p.value) # If closer to Berkeley Marina, use that p-value
  
  todays_results$ks_pval[i] <- 
  todays_flags$flag_KS_now[i] <- ifelse(ks_p <= 0.05, 1, 0) #If p-value less than 0.05 add flag, else flag = 0
  
  ## Flag via mean and variance
  var_aqy <- var(aqy_O3_60$O3) # Variance AQY data
  var_sp <- var(sp_O3$RawConcentration) # Variance regulatory data

  mean_aqy <- mean(aqy_O3_60$O3) # Mean AQY data
  mean_sp <- mean(sp_O3$RawConcentration) # Mean regulatory data
  
  # Flag via slope drift
  manual_slope <- sqrt(var_sp/var_aqy)
  todays_results$slope[i] <- manual_slope
  todays_flags$flag_slope_now[i] <- ifelse(manual_slope > 1.3 | manual_slope < .7, 1, 0) # Assign flag if manual slope outside bounds, else flag = 0
  
  # Flag via offset drift
  manual_offset <- (mean_aqy-mean_sp)*sqrt(var_sp/var_aqy)
  todays_results$offset[i] <- manual_offset
  todays_flags$flag_offset_now[i] <- ifelse(manual_offset > 5 | manual_offset < 5, 1, 0) # Assign flag if manual offset outside bounds, else flag = 0
  
  ## Print parameters
  print(paste('KS P-VALUE:', ks_results$p.value, '| MANUAL SLOPE:', manual_slope, '| MANUAL OFFSET:', manual_offset))
  i <- i + 1
}
```

```{r}
# LOG KS TEST RESULTS, SLOPE, AND OFFSET DAILY
flag_params <- read.csv('results/flag_parameters.csv', header = T)

### COMBINE WITH PREVIOUS FLAGS
running_flags <- read.csv('results/running_flags.csv') %>% full_join(., todays_flags, by = 'Monitor.ID') #Combine running flags with today's flags

running_flags <- mutate(running_flags, #If today's flag = 0, cumulative flag resets to zero - if today's flag = 1, add to previous cumulative total
       flag_KS_run = ifelse(flag_KS_now == 0, 0, flag_KS_run + flag_KS_now),
       flag_slope_run = ifelse(flag_slope_now == 0, 0, flag_slope_run + flag_slope_now),
       flag_offset_run = ifelse(flag_offset_now == 0, 0, flag_offset_run + flag_offset_now))

#write.csv('results/running_flags.csv') #Write the new running flags to csv
```

```{r}
#read.csv('results/running_flags.csv') # Read in running flags


```










































