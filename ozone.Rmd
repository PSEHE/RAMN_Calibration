---
title: "Remote Network Calibration"
author: "Audrey Smith"
date: "6/9/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(httr)
library(jsonlite)
library(dplyr)
library(lubridate)
library(ggplot2)
```

* This script reads in un-calibrated network data that has been pulled from the Aeroqual API via Python script. 
* This script requires a proxy to calibrate our ozone data. San Pablo Station regulatory data is pulled for these purposes and used as a proxy for all stations, given the regional nature of ozone pollution. Proximity is more important than land use here, justifying the choice of one location for proxy data.
  -Later, will test results of using just San Pablo station versus pulling air monitoring data from nearby monitors, including Aquatic Park, West Oakland, San Rafael, Vallejo, etc.
    -https://www.baaqmd.gov/about-air-quality/current-air-quality/air-monitoring-data/#/airp?id=59&style=chart&zone=-1&date=2020-06-09&view=hourly
    -Feasible some stations may be equidistant between San Pablo and Berkeley Aquatic Park - calculate distance between each monitor and these stations to verify SP closest for all
       -If not, consider integrating Berkeley data for south Richmond stations

1. Read in data pulled daily by Blue's script

2. Call regulatory API and wrangle their data into same format as ours
   *** AirNow API provides unverified regulatory data in real time: https://docs.airnowapi.org/Data/query ***
   * EPA Air Quality System API provides authoritative regulatory data (but up to 6 month lag): https://aqs.epa.gov/aqsweb/documents/data_api.html
   * World AQI collaboration between global EPAs provides regulatory data by station in real time: https://aqicn.org/api/
   ?? Do we know of somewhere to get validated data with less lag? Will have to use AirNow data for now but should have a retroactive check in place maybe
   2.a. Each day, pull in just the previous day and append to a running log of proxy data that is written as output every day

3.a. Run Kolmogorov Smirnov test to compare three-day rolling average of each monitor to that of the BAAQMD data
    3.a.i. A flag should be created for each time KS statistic goes below 0.05 (X = 1)
        3.a.ii. Create separate column for running, five-day count of flag (Y). If X_yesterday == 1 & X_today == 1, Y = sum(Y_yesterday+Y_today). Else, Y_today = 0.

3.b. Run deviance test to see if mean and variance of our data differ substantially with those of the proxy data
    3.b.i. A separate flag should be created for each of these, follow same scheme for triggering re-calibration as detailed under 3.a.
    
3.c. Each day, write a running .csv with flag results
    3.c.i. Next day, read in just one day of new data and append to the previous day's running file

4. If Y < 5, pass. If Y == 5, re-calibrate.
    4.a. Calibration uses mean and variance of our data, proxy data to create new slope and offset
        * I understand the calibration to be applied to day 6 onward, need to verify that it doesn't begin to apply to day 5 or apply retroactively at day 1
        ** Calibration uses mean and variance for 30-day period, beginning on day 5 and going backwards
        
5. Need to create log of what day calibration parameter changed, how many parameters were created for any given monitor, and what they were
    5.i. Create separate CSV logging these values for each monitor
  
    
```{r}
### DOWNLOAD PROXY DATA
# Get last three days of regulatory data  near Richmond
#end_date <- Sys.Date()-1 #Yesterday
#start_date <- Sys.Date()-3 #Three days ago
start_date <- '2019-12-10'
end_date <- '2019-12-12'

request_url = paste0('http://www.airnowapi.org/aq/data/?startDate=', start_date, 'T00&endDate=', end_date, 'T23&parameters=OZONE,PM25,NO2&BBOX=-122.720038,37.701918,-121.965229,38.126622&dataType=C&format=application/json&verbose=0&nowcastonly=0&includerawconcentrations=1&API_KEY=C05358E3-5508-4216-A03E-E229E0368B7E') # Set URL to request data from last three days from AirNow API ## DATES NEEDED AS FUNCTION INPUT HERE
request_call <- GET(url = request_url) #Request data from API
stop_for_status(request_call) # Converts API error to R error - fails if invalid query or data unavailable
request_json <- content(request_call, as = 'text', type = NULL, encoding = 'UTF-8') # Get JSON content from API request as text
request_df <- fromJSON(request_json, simplifyDataFrame = TRUE) # Parse JSON to dataframe

# Filter to O3 and join with station names, filter to San Pablo
regulatory_stations <- read.csv('shapefiles/monitor_shp_named.csv') %>% select(-X) %>% rename('station'='monitor_names') # Read in names of each regulatory station near Richmond
regulatory_all <- inner_join(request_df, regulatory_stations, by = c('Latitude'='monitor_lat', 'Longitude'='monitor_long')) # Join station names with data by lat/long
sp_O3 <- filter(regulatory_all, Parameter == 'OZONE' & station == 'San Pablo') # Filter to pollutant and station of interest - can include others if we wish ## FUNCTION INPUT
sp_O3$RawConcentration <- ifelse(sp_O3$RawConcentration == -999, sp_O3$Value, sp_O3$RawConcentration) # If no raw concentration available, use rounded value from 'Value' col
sp_O3$time_stamp <- as_datetime(paste(substr(sp_O3$UTC, 1, 10), substr(sp_O3$UTC, 12, 16), ':00')) # Get time into standardized format
```

### READ IN PSE MINUTE DATA FROM DAY PRIOR
For now we will average the minute data into sixty minute increments for more comparable sample size to proxy data - do we have access to better temporal granularity for proxy data?
```{r}
aqy_data_full <- read.csv(paste0('test_data/PSE_AQY_minute_2019-12-06_', '2020-06-10', '.CSV'), stringsAsFactors = F) # Read in our air data
aqy_cal_params <- read.csv('calParams_sac.csv') # Read in present calibration parameters

#Filtering to last three days
aqy_data_full$time_stamp <- as_datetime(aqy_data_full$Time) # Get timestamp into standard format
aqy_data <- filter(aqy_data_full, time_stamp >= start_date & time_stamp <= end_date) # Filter to timestamps within last 72 hours

#Calibrating using current parameters
aqy_data_cal <- inner_join(aqy_cal_params, aqy_data, by = c('Monitor.ID'='ID')) # Join uncalibrated data with present calibration parameters
aqy_data_cal$O3_cal <- (aqy_data_cal$O3-aqy_data_cal$O3.offset)*aqy_data_cal$O3.gain # Calibrate O3 data

# Generating list of AQYs
aqy_list <- as.data.frame(unique(aqy_data_cal$Monitor.ID)) # Generate list of AQY instruments with measurements in time frame
colnames(aqy_list) <- 'Monitor.ID'

# Filtering to one AQY
aqy_O3 <- filter(aqy_data_cal, Monitor.ID == aqy_list[1,1]) # Select AQY of interest ## WILL NEED TO ITERATE HERE - SUBSEQUENT STEPS NESTED IN FOR LOOP

#Averaging by hour
aqy_O3$day_hour <- paste(day(aqy_O3$Time), hour(aqy_O3$Time)) # Generate column with unique ID for day/hour
aqy_O3_60 <- group_by(aqy_O3, Monitor.ID, day_hour) %>% summarize(O3 = mean(O3_cal)) # Group by day/hour and take average of minute measurements
```

```{r}
### COMPARE DISTRIBUTIONS VISUALLY
var_aqy <- var(aqy_O3_60$O3) # Variance AQY data
var_sp <- var(sp_O3$RawConcentration) # Variance regulatory data
print(paste('AQY variance:', var_aqy, '|', 'Regulatory Variance:', var_sp))

mean_aqy <- mean(aqy_O3_60$O3) # Mean AQY data
mean_sp <- mean(sp_O3$RawConcentration) # Mean regulatory data

ggplot() + geom_bar(data = sp_O3, aes(x = RawConcentration), binwidth = 1, alpha = 1) + geom_bar(data = aqy_O3_60, aes(x = O3), binwidth = 1, alpha = .5, fill = 'red') +
  ggtitle(paste('Proxy Data (black) & AQY Data (red)\n', start_date, 'to', end_date)) + xlab('\nConcentration O3 (ppb)') + ylab('72-Hour Count of Measurements \n') +
  theme(plot.title = element_text(hjust = .5, face = 'bold'), axis.title = element_text(size = 9, face = 'italic')) # Plot distributions of each dataset superimposed
```

Miskell et al. 2016 indicates:
slope = sqrt(variance_proxy/variance_aqy)
offset = mean_aqy-mean_proxy*sqrt(variance_proxy/variance_aqy)

Says drift has occurred if 0.7 < slope < 1.3 or -5 < offset < 5 ??Are the greater than/less than signs flipped??
```{r}
### GENERATE FLAGS
## Initialize blank columns for flags
aqy_flags <- mutate(aqy_list, flag_KS = 0, flag_slope = 0, flag_offset = 0)

## Flag via KS test
ks_results <- ks.test(sp_O3$RawConcentration, aqy_O3_60$O3, alternative = 'two.sided') # Kolmogorov-Smirnov test to check whether distributions are different

aqy_flags$flag_KS[1] <- ifelse(ks_results$p.value <= 0.05, 1, 0) #If p-value less than 0.05, add flag

## Flag via slope drift
manual_slope <- sqrt(var_sp/var_aqy)
aqy_flags$flag_slope[1] <- ifelse(manual_slope > 1.3 | manual_slope < .7, 1, 0)

## Flag via offset drift
manual_offset <- (mean_aqy-mean_sp)*sqrt(var_sp/var_aqy)
aqy_flags$flag_offset[1] <- ifelse(manual_offset > 5 | manual_offset < 5, 1, 0)

## Print parameters
print(paste('KS P-VALUE:', ks_results$p.value, '| MANUAL SLOPE:', manual_slope, '| MANUAL OFFSET:', manual_offset))
aqy_flags[1,]
```

```{r}
aqy_flags
```



















































